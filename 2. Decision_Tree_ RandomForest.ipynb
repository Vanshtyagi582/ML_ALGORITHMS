{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Decision Tree and Bagging Classifier Example\n",
    "\n",
    "This notebook explains a custom implementation of a Decision Tree Classifier and a Bagging Classifier. It shows how the models are built, trained, and used for predictions using a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "node_dt_intro",
   "metadata": {},
   "source": [
    "## Node and Decision Tree Classifier\n",
    "\n",
    "We define a `Node` class to represent each node in the tree and a `DecisionTreeClassifier` class that handles tree construction, splitting, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "node_dt_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, feature_subset=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.feature_subset = feature_subset\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert pandas DataFrame/Series to numpy array\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "            y = y.values.flatten()\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y)) if n_samples > 0 else 0\n",
    "\n",
    "        # Stopping conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (n_samples < self.min_samples_split) or \\\n",
    "           (n_classes <= 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Find best split using allowed features\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Split data\n",
    "        left_idxs = X[:, best_feature] <= best_threshold\n",
    "        right_idxs = ~left_idxs\n",
    "\n",
    "        left = self._build_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right = self._build_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "\n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        # Use subset of features if specified\n",
    "        if self.feature_subset is not None:\n",
    "            features_to_consider = self.feature_subset\n",
    "        else:\n",
    "            features_to_consider = range(n_features)\n",
    "\n",
    "        for feature_idx in features_to_consider:\n",
    "            feature_values = X[:, feature_idx]\n",
    "            unique_values = np.unique(feature_values)\n",
    "\n",
    "            if len(unique_values) <= 1:\n",
    "                continue\n",
    "\n",
    "            thresholds = [(unique_values[i] + unique_values[i+1])/2 for i in range(len(unique_values)-1)]\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_idxs = feature_values <= threshold\n",
    "                right_idxs = ~left_idxs\n",
    "\n",
    "                if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini = self._gini(y[left_idxs], y[right_idxs])\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _gini(self, left_y, right_y):\n",
    "        n_left = len(left_y)\n",
    "        n_right = len(right_y)\n",
    "        n_total = n_left + n_right\n",
    "\n",
    "        if n_total == 0:\n",
    "            return 0.0\n",
    "\n",
    "        gini_left = 1.0 - sum((np.sum(left_y == c)/n_left)**2 for c in np.unique(left_y)) if n_left !=0 else 0\n",
    "        gini_right = 1.0 - sum((np.sum(right_y == c)/n_right)**2 for c in np.unique(right_y)) if n_right !=0 else 0\n",
    "\n",
    "        return (n_left/n_total)*gini_left + (n_right/n_total)*gini_right\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        return np.argmax(np.bincount(y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "\n",
    "    def _predict_single(self, x, node=None):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._predict_single(x, node.left)\n",
    "        else:\n",
    "            return self._predict_single(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bagging_intro",
   "metadata": {},
   "source": [
    "## Bagging Classifier\n",
    "\n",
    "The `BaggingClassifier` class builds multiple trees using bootstrap samples of the data. It uses the out-of-bag (OOB) samples to estimate prediction error and supports random feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bagging_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingClassifier:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.estimators = []\n",
    "        self.oob_indices = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        self.estimators = []\n",
    "        self.oob_indices = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Create bootstrap sample\n",
    "            bootstrap_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            oob_mask = np.zeros(n_samples, dtype=bool)\n",
    "            oob_mask[bootstrap_indices] = True\n",
    "            oob_indices = np.where(~oob_mask)[0]\n",
    "            self.oob_indices.append(oob_indices)\n",
    "\n",
    "            X_boot = X.iloc[bootstrap_indices] if isinstance(X, pd.DataFrame) else X[bootstrap_indices]\n",
    "            y_boot = y.iloc[bootstrap_indices] if isinstance(y, pd.Series) else y[bootstrap_indices]\n",
    "\n",
    "            # Feature selection\n",
    "            feature_subset = None\n",
    "            if self.max_features is not None:\n",
    "                feature_subset = np.random.choice(n_features, self.max_features, replace=False)\n",
    "\n",
    "            # Train tree\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                feature_subset=feature_subset\n",
    "            )\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        all_preds = np.zeros((X.shape[0], self.n_estimators))\n",
    "        for i, tree in enumerate(self.estimators):\n",
    "            all_preds[:, i] = tree.predict(X)\n",
    "        return np.array([np.argmax(np.bincount(row.astype(int))) for row in all_preds])\n",
    "\n",
    "    def compute_oob_error(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        oob_preds = defaultdict(list)\n",
    "\n",
    "        for i, tree in enumerate(self.estimators):\n",
    "            oob_idx = self.oob_indices[i]\n",
    "            if len(oob_idx) == 0:\n",
    "                continue\n",
    "            X_oob = X.iloc[oob_idx] if isinstance(X, pd.DataFrame) else X[oob_idx]\n",
    "            preds = tree.predict(X_oob)\n",
    "            for idx, pred in zip(oob_idx, preds):\n",
    "                oob_preds[idx].append(pred)\n",
    "\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for idx in range(n_samples):\n",
    "            if idx in oob_preds and len(oob_preds[idx]) > 0:\n",
    "                majority_vote = np.argmax(np.bincount(oob_preds[idx]))\n",
    "                y_pred.append(majority_vote)\n",
    "                y_true.append(y.iloc[idx] if isinstance(y, pd.Series) else y[idx])\n",
    "\n",
    "        return np.mean(np.array(y_pred) != np.array(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_print_tree",
   "metadata": {},
   "source": [
    "## Helper Function: Print Tree\n",
    "\n",
    "The `print_tree` function is used to visualize the decision tree structure with feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "print_tree_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, feature_names=None, depth=0):\n",
    "    if feature_names is None:\n",
    "        feature_names = ['Age', 'Income', 'Student', 'Credit Rating']\n",
    "    indent = \"  \" * depth\n",
    "    if node.value is not None:\n",
    "        print(f\"{indent}Leaf: Class {node.value}\")\n",
    "        return\n",
    "    print(f\"{indent}Feature: {feature_names[node.feature_index]} <= {node.threshold:.2f}\")\n",
    "    print(f\"{indent}--> True:\")\n",
    "    print_tree(node.left, feature_names, depth+1)\n",
    "    print(f\"{indent}--> False:\")\n",
    "    print_tree(node.right, feature_names, depth+1)\n",
    "\n",
    "# This function recursively prints the tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_preprocessing",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We create a sample dataset, then encode the categorical features into numeric values for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "data_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
    "    'Income': ['High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High'],\n",
    "    'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No'],\n",
    "    'Credit Rating': ['Fair', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair'],\n",
    "    'Buy Computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical features\n",
    "df['Income'] = df['Income'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "df['Student'] = df['Student'].map({'No': 0, 'Yes': 1})\n",
    "df['Credit Rating'] = df['Credit Rating'].map({'Fair': 0, 'Excellent': 1})\n",
    "df['Buy Computer'] = df['Buy Computer'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "X = df.drop('Buy Computer', axis=1)\n",
    "y = df['Buy Computer']\n",
    "\n",
    "# This cell prepares the data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original_tree",
   "metadata": {},
   "source": [
    "## Training the Original Decision Tree\n",
    "\n",
    "We train a single decision tree without bagging and compute its training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "original_tree_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of Original Decision Tree: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Original Decision Tree\n",
    "original_tree = DecisionTreeClassifier(max_depth=3)\n",
    "original_tree.fit(X, y)\n",
    "original_prediction = original_tree.predict(X.values)\n",
    "train_accuracy = np.mean(original_prediction == y)\n",
    "\n",
    "print(\"Training Accuracy of Original Decision Tree:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bagging_all_features",
   "metadata": {},
   "source": [
    "## Bagging with All Features\n",
    "\n",
    "Next, we train a bagging ensemble using all features. We compute the out-of-bag (OOB) error as an estimate of generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bagging_all_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Error (All Features): 0.375\n"
     ]
    }
   ],
   "source": [
    "# Bagging with all features\n",
    "bagging_all = BaggingClassifier(n_estimators=10, max_depth=3, max_features=None)\n",
    "bagging_all.fit(X, y)\n",
    "oob_error_all = bagging_all.compute_oob_error(X, y)\n",
    "\n",
    "print(\"OOB Error (All Features):\", oob_error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bagging_two_features",
   "metadata": {},
   "source": [
    "## Bagging with 2 Random Features\n",
    "\n",
    "We repeat the bagging process using only 2 randomly selected features for each tree and compute its OOB error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bagging_two_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Error (2 Random Features): 0.375\n"
     ]
    }
   ],
   "source": [
    "# Bagging with 2 random features\n",
    "bagging_two = BaggingClassifier(n_estimators=10, max_depth=3, max_features=2)\n",
    "bagging_two.fit(X, y)\n",
    "oob_error_two = bagging_two.compute_oob_error(X, y)\n",
    "\n",
    "print(\"OOB Error (2 Random Features):\", oob_error_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_data_prediction",
   "metadata": {},
   "source": [
    "## New Data Prediction and Results\n",
    "\n",
    "Finally, we create a new data point (Age=42, Income=Low, Student=No, Credit Rating=Excellent) and get predictions from the original tree and both bagging ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "new_data_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Original Decision Tree (No Bagging)\n",
      "Prediction (Age=42): Yes\n",
      "\n",
      "=============================================\n",
      "Bagging (All Features)\n",
      "OOB Error: 0.3750\n",
      "Prediction (Age=42): Yes\n",
      "\n",
      "=============================================\n",
      "Bagging (2 Random Features)\n",
      "OOB Error: 0.3750\n",
      "Prediction (Age=42): No\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# New data point for prediction\n",
    "new_data = pd.DataFrame([[42, 0, 0, 1]], columns=X.columns)\n",
    "\n",
    "print(\"=\"*45)\n",
    "print(\"Original Decision Tree (No Bagging)\")\n",
    "print(f\"Prediction (Age=42): {'Yes' if original_tree.predict(new_data.values)[0] == 1 else 'No'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(\"Bagging (All Features)\")\n",
    "print(f\"OOB Error: {oob_error_all:.4f}\")\n",
    "print(f\"Prediction (Age=42): {'Yes' if bagging_all.predict(new_data.values)[0] == 1 else 'No'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(\"Bagging (2 Random Features)\")\n",
    "print(f\"OOB Error: {oob_error_two:.4f}\")\n",
    "print(f\"Prediction (Age=42): {'Yes' if bagging_two.predict(new_data.values)[0] == 1 else 'No'}\")\n",
    "print(\"=\"*45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
